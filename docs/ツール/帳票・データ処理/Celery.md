# Celery

## æ¦‚è¦

**Celery**ã¯ã€Pythonãƒ™ãƒ¼ã‚¹ã®åˆ†æ•£ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚éåŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã«ã‚ˆã‚Šã€Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡ã„å‡¦ç†ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

## åŸºæœ¬æƒ…å ±

| é …ç›® | å†…å®¹ |
|------|------|
| **é–‹ç™ºå…ƒ** | Ask Solem Hoel / ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ |
| **ç¨®åˆ¥** | åˆ†æ•£ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ãƒ»éåŒæœŸå‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ |
| **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** | BSD Licenseï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ï¼‰ |
| **æ–™é‡‘** | ğŸŸ¢ ç„¡æ–™ |
| **å…¬å¼ã‚µã‚¤ãƒˆ** | https://docs.celeryq.dev/ |
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ** | https://docs.celeryq.dev/en/stable/ |

## ä¸»ãªç‰¹å¾´

### 1. éåŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
- **é…å»¶å®Ÿè¡Œ**: `task.delay()`ã§å³åº§ã«ãƒªã‚¿ãƒ¼ãƒ³
- **çµæœå–å¾—**: AsyncResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§çŠ¶æ…‹ç›£è¦–
- **ãƒã‚§ã‚¤ãƒ³ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—**: ã‚¿ã‚¹ã‚¯ã®é€£é–ãƒ»ä¸¦åˆ—å®Ÿè¡Œ
- **å„ªå…ˆåº¦åˆ¶å¾¡**: ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦è¨­å®š

### 2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼å¯¾å¿œ
- **Redis**: é«˜é€Ÿã€é–‹ç™ºãƒ»æœ¬ç•ªæ¨å¥¨
- **RabbitMQ**: é«˜æ©Ÿèƒ½ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘
- **Amazon SQS**: ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–
- **ãã®ä»–**: Kafkaã€ZeroMQå¯¾å¿œ

### 3. å®šæœŸå®Ÿè¡Œï¼ˆCelery Beatï¼‰
- **Crontab**: cronå½¢å¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
- **Interval**: å®šæœŸå®Ÿè¡Œï¼ˆç§’/åˆ†/æ™‚é–“ï¼‰
- **Solar**: æ—¥ã®å‡ºãƒ»æ—¥ã®å…¥ã‚Šãƒ™ãƒ¼ã‚¹
- **å‹•çš„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**: Django Adminé€£æº

### 4. çµæœãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- **Redis**: é«˜é€Ÿã€æ¨å¥¨
- **Database**: SQLAlchemyã€Django ORM
- **Memcached**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- **Elasticsearch**: æ¤œç´¢ãƒ»åˆ†æ

## ä½¿ã„æ–¹

### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Celeryã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆRedisä½¿ç”¨ï¼‰
pip install celery[redis]

# ã¾ãŸã¯RabbitMQä½¿ç”¨
pip install celery[amqp]

# ãã®ä»–ä¾å­˜é–¢ä¿‚
pip install redis  # Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
```

### åŸºæœ¬è¨­å®š

```python
# celery_app.py
from celery import Celery

# Celeryã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
app = Celery(
    'myapp',
    broker='redis://localhost:6379/0',  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼
    backend='redis://localhost:6379/0'  # çµæœãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
)

# è¨­å®š
app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Tokyo',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30åˆ†
    task_soft_time_limit=25 * 60,  # 25åˆ†ï¼ˆè­¦å‘Šï¼‰
)
```

### ã‚¿ã‚¹ã‚¯å®šç¾©

```python
# tasks.py
from celery_app import app
import time
import requests

@app.task
def add(x, y):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯"""
    return x + y

@app.task(bind=True)
def long_task(self, iterations):
    """é€²æ—ã‚’å ±å‘Šã™ã‚‹ã‚¿ã‚¹ã‚¯"""
    for i in range(iterations):
        time.sleep(1)
        self.update_state(
            state='PROGRESS',
            meta={'current': i, 'total': iterations}
        )
    return {'current': iterations, 'total': iterations, 'status': 'Complete!'}

@app.task(bind=True, max_retries=3)
def fetch_url(self, url):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã‚¿ã‚¹ã‚¯"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.RequestException as exc:
        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
        raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries))

@app.task
def send_email(to, subject, body):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¿ã‚¹ã‚¯"""
    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡å‡¦ç†
    print(f"Sending email to {to}: {subject}")
    return f"Email sent to {to}"
```

### ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ

```python
# main.py
from tasks import add, long_task, fetch_url, send_email

# éåŒæœŸå®Ÿè¡Œ
result = add.delay(4, 6)
print(f"Task ID: {result.id}")

# çµæœå–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
print(f"Result: {result.get(timeout=10)}")

# çŠ¶æ…‹ç¢ºèª
if result.ready():
    print("Task completed")
    print(f"Result: {result.result}")
elif result.failed():
    print("Task failed")
    print(f"Error: {result.traceback}")
else:
    print("Task pending or running")

# é€²æ—ç›£è¦–
task = long_task.delay(100)
while not task.ready():
    if task.state == 'PROGRESS':
        meta = task.info
        print(f"Progress: {meta['current']}/{meta['total']}")
    time.sleep(1)

# è¤‡æ•°ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
results = [add.delay(i, i) for i in range(10)]
for result in results:
    print(result.get())
```

### Celery Workerèµ·å‹•

```bash
# Workerãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•
celery -A celery_app worker --loglevel=info

# è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
celery -A celery_app worker --concurrency=4

# ãƒ‡ãƒ¼ãƒ¢ãƒ³åŒ–ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
celery -A celery_app worker --detach --loglevel=info --logfile=celery.log

# ç‰¹å®šã‚­ãƒ¥ãƒ¼ã®ã¿å‡¦ç†
celery -A celery_app worker -Q high_priority,default

# Autoscaleï¼ˆå‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ï¼‰
celery -A celery_app worker --autoscale=10,3  # max 10, min 3
```

### Celery Beatï¼ˆå®šæœŸå®Ÿè¡Œï¼‰

```python
# celery_app.py
from celery import Celery
from celery.schedules import crontab

app = Celery('myapp', broker='redis://localhost:6379/0')

app.conf.beat_schedule = {
    'add-every-30-seconds': {
        'task': 'tasks.add',
        'schedule': 30.0,  # 30ç§’ã”ã¨
        'args': (16, 16)
    },
    'send-report-every-monday': {
        'task': 'tasks.send_weekly_report',
        'schedule': crontab(hour=9, minute=0, day_of_week=1),  # æ¯é€±æœˆæ›œ 9:00
    },
    'cleanup-every-night': {
        'task': 'tasks.cleanup_old_data',
        'schedule': crontab(hour=2, minute=0),  # æ¯æ—¥ 2:00
    },
}

app.conf.timezone = 'Asia/Tokyo'
```

```bash
# Beatã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼èµ·å‹•
celery -A celery_app beat --loglevel=info

# Worker + BeatåŒæ™‚èµ·å‹•ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
celery -A celery_app worker -B --loglevel=info
```

### Djangoçµ±åˆ

```python
# myproject/celery.py
from celery import Celery
import os

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproject.settings')

app = Celery('myproject')
app.config_from_object('django.conf:settings', namespace='CELERY')
app.autodiscover_tasks()

@app.task(bind=True)
def debug_task(self):
    print(f'Request: {self.request!r}')
```

```python
# myproject/__init__.py
from .celery import app as celery_app

__all__ = ('celery_app',)
```

```python
# settings.py
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'
CELERY_RESULT_SERIALIZER = 'json'
CELERY_TIMEZONE = 'Asia/Tokyo'
```

```python
# myapp/tasks.py
from celery import shared_task
from django.core.mail import send_mail

@shared_task
def send_notification(user_id, message):
    from .models import User
    user = User.objects.get(id=user_id)
    send_mail(
        'Notification',
        message,
        'noreply@example.com',
        [user.email],
    )
    return f"Sent to {user.email}"
```

### Flaskçµ±åˆ

```python
# app.py
from flask import Flask
from celery import Celery

def make_celery(app):
    celery = Celery(
        app.import_name,
        broker=app.config['CELERY_BROKER_URL'],
        backend=app.config['CELERY_RESULT_BACKEND']
    )
    celery.conf.update(app.config)
    return celery

app = Flask(__name__)
app.config['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'
app.config['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379/0'

celery = make_celery(app)

@celery.task
def process_data(data):
    # é‡ã„å‡¦ç†
    import time
    time.sleep(10)
    return f"Processed {len(data)} items"

@app.route('/process', methods=['POST'])
def process():
    data = request.json.get('data', [])
    task = process_data.delay(data)
    return {'task_id': task.id}, 202
```

### ãƒã‚§ã‚¤ãƒ³ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—

```python
# tasks.py
from celery import chain, group, chord
from celery_app import app

@app.task
def multiply(x, y):
    return x * y

@app.task
def add(x, y):
    return x + y

@app.task
def summarize(results):
    return sum(results)

# ãƒã‚§ã‚¤ãƒ³ï¼ˆé †æ¬¡å®Ÿè¡Œï¼‰
result = chain(add.s(2, 2), multiply.s(4)).apply_async()
# (2 + 2) * 4 = 16
print(result.get())

# ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
job = group([
    add.s(2, 2),
    add.s(4, 4),
    add.s(8, 8),
])
result = job.apply_async()
print(result.get())  # [4, 8, 16]

# Chordï¼ˆä¸¦åˆ—å®Ÿè¡Œ â†’ çµæœã‚’é›†ç´„ï¼‰
callback = summarize.s()
header = group([add.s(i, i) for i in range(10)])
result = chord(header)(callback)
print(result.get())  # 90
```

### ç›£è¦–ï¼ˆFlowerï¼‰

```bash
# Flowerã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install flower

# Flowerèµ·å‹•
celery -A celery_app flower

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹
# http://localhost:5555
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
# tasks.py
from celery import Task
from celery_app import app

class CallbackTask(Task):
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã‚¯ãƒ©ã‚¹"""
    def on_success(self, retval, task_id, args, kwargs):
        print(f"Task {task_id} succeeded: {retval}")

    def on_failure(self, exc, task_id, args, kwargs, einfo):
        print(f"Task {task_id} failed: {exc}")

    def on_retry(self, exc, task_id, args, kwargs, einfo):
        print(f"Task {task_id} retrying: {exc}")

@app.task(base=CallbackTask, bind=True, max_retries=3)
def risky_task(self, data):
    try:
        # ãƒªã‚¹ã‚¯ã®ã‚ã‚‹å‡¦ç†
        if not data:
            raise ValueError("Empty data")
        return process(data)
    except Exception as exc:
        self.retry(exc=exc, countdown=60)
```

### Dockeræ§‹æˆ

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  web:
    build: .
    command: python app.py
    ports:
      - "5000:5000"
    depends_on:
      - redis
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0

  celery_worker:
    build: .
    command: celery -A celery_app worker --loglevel=info
    depends_on:
      - redis
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0

  celery_beat:
    build: .
    command: celery -A celery_app beat --loglevel=info
    depends_on:
      - redis
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0

  flower:
    build: .
    command: celery -A celery_app flower
    ports:
      - "5555:5555"
    depends_on:
      - redis
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
```

## é–‹ç™ºå·¥ç¨‹ã§ã®åˆ©ç”¨

| å·¥ç¨‹ | ç”¨é€” | è©³ç´° |
|------|------|------|
| **å®Ÿè£…** | éåŒæœŸå‡¦ç† | é‡ã„å‡¦ç†ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ |
| **å®Ÿè£…** | ãƒãƒƒãƒå‡¦ç† | ãƒ‡ãƒ¼ã‚¿é›†è¨ˆãƒ»å¤‰æ›å‡¦ç† |
| **é‹ç”¨** | å®šæœŸå®Ÿè¡Œ | ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— |
| **é‹ç”¨** | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° | è² è·åˆ†æ•£ã€æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |

## ãƒ¡ãƒªãƒƒãƒˆ

- **éåŒæœŸå‡¦ç†**: Webãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®é«˜é€ŸåŒ–
- **åˆ†æ•£å®Ÿè¡Œ**: è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã§è² è·åˆ†æ•£
- **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°**: Cronä»£æ›¿ã€æŸ”è»Ÿãªå®šæœŸå®Ÿè¡Œ
- **ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½**: å¤±æ•—æ™‚ã®è‡ªå‹•å†è©¦è¡Œ
- **ç›£è¦–ãƒ„ãƒ¼ãƒ«**: Flowerã€Prometheusçµ±åˆ
- **è¨€èªçµ±åˆ**: Pythonã€Djangoã€Flask
- **ç„¡æ–™**: ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹

## ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

- **ã‚¤ãƒ³ãƒ•ãƒ©è¤‡é›‘åŒ–**: Redis/RabbitMQå¿…é ˆ
- **ãƒ‡ãƒãƒƒã‚°å›°é›£**: éåŒæœŸå‡¦ç†ã®ãƒ‡ãƒãƒƒã‚°
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å¤§é‡ã‚¿ã‚¹ã‚¯ã§ãƒ¡ãƒ¢ãƒªæ¶ˆè²»
- **ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºåˆ¶é™**: JSONå¯¾å¿œå‹ã®ã¿
- **Pythonå°‚ç”¨**: ä»–è¨€èªã¯åˆ¥ãƒ„ãƒ¼ãƒ«å¿…è¦
- **è¨­å®šè¤‡é›‘**: æœ¬ç•ªç’°å¢ƒã®æœ€é©åŒ–ãŒé›£ã—ã„

## é¡ä¼¼ãƒ„ãƒ¼ãƒ«ã¨ã®æ¯”è¼ƒ

| ãƒ„ãƒ¼ãƒ« | ç‰¹å¾´ | æ–™é‡‘ | é©ç”¨å ´é¢ |
|--------|------|------|----------|
| **Celery** | Pythonã€åˆ†æ•£ã€è±Šå¯Œãªæ©Ÿèƒ½ | ç„¡æ–™ | Djangoã‚¢ãƒ—ãƒªã€ãƒ‡ãƒ¼ã‚¿å‡¦ç† |
| **RQ (Redis Queue)** | ã‚·ãƒ³ãƒ—ãƒ«ã€Rediså°‚ç”¨ | ç„¡æ–™ | è»½é‡ã‚¿ã‚¹ã‚¯ã€Flask |
| **Dramatiq** | ãƒ¢ãƒ€ãƒ³ã€é«˜é€Ÿ | ç„¡æ–™ | Pythonæ±ç”¨ |
| **AWS SQS + Lambda** | ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã€ãƒãƒãƒ¼ã‚¸ãƒ‰ | å¾“é‡èª²é‡‘ | ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ– |

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ã‚¿ã‚¹ã‚¯è¨­è¨ˆ

```python
# å†ªç­‰æ€§ã‚’ä¿ã¤
@app.task
def process_order(order_id):
    order = Order.objects.get(id=order_id)
    if order.status == 'processed':
        return 'Already processed'
    # å‡¦ç†
    order.status = 'processed'
    order.save()
```

### 2. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š

```python
# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ»ãƒªãƒˆãƒ©ã‚¤è¨­å®š
@app.task(
    time_limit=300,  # 5åˆ†ã§KILL
    soft_time_limit=240,  # 4åˆ†ã§è­¦å‘Š
    max_retries=3,
    default_retry_delay=60
)
def long_running_task():
    pass
```

### 3. å„ªå…ˆåº¦ã‚­ãƒ¥ãƒ¼

```python
# è¨­å®š
app.conf.task_routes = {
    'tasks.high_priority_task': {'queue': 'high'},
    'tasks.low_priority_task': {'queue': 'low'},
}

# Workerèµ·å‹•
# celery -A celery_app worker -Q high,default
# celery -A celery_app worker -Q low
```

### 4. ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°

```python
# ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ç›£è¦–
from celery.signals import task_prerun, task_postrun
import time

@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, **kwargs):
    print(f"Task {task_id} started")

@task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, **kwargs):
    print(f"Task {task_id} completed")
```

## å…¬å¼ãƒªã‚½ãƒ¼ã‚¹

- **å…¬å¼ã‚µã‚¤ãƒˆ**: https://docs.celeryq.dev/
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.celeryq.dev/en/stable/
- **GitHub**: https://github.com/celery/celery
- **Flower**: https://flower.readthedocs.io/
- **ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«**: https://docs.celeryq.dev/en/stable/getting-started/

## ã¾ã¨ã‚

Celeryã¯ã€Pythonãƒ™ãƒ¼ã‚¹ã®åˆ†æ•£ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚éåŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œã€å®šæœŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã«ã‚ˆã‚Šã€Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡ã„å‡¦ç†ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚Djangoãƒ»Flaskçµ±åˆã€è±Šå¯Œãªç›£è¦–ãƒ„ãƒ¼ãƒ«ã«ã‚ˆã‚Šã€æœ¬ç•ªç’°å¢ƒã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¿ã‚¹ã‚¯å‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

---

**æœ€çµ‚æ›´æ–°**: 2025-12-10
**å¯¾è±¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: Celery 5.3+
